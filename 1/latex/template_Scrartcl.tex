\documentclass[11pt]{scrartcl}
\usepackage[top=1cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{graphicx,float}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
%opening
\title{Project Report 1}
\author{Dingyi Zhuang, Tianyu Shi, Fuyuan Lyu}

\begin{document}

\maketitle
\begin{abstract}
<<<<<<< HEAD
In this project, we implement two basic machine learning models: logistic regression and naive bayes. These models are tested upon four classification datasets: Ionosphere Dataset, Iris Dataset, Car Evaluation Dataset and Adult Dataset. In the pre-processing phase, we first discretize all the continuous features, handling the missing values and values out of range. In the experiment phase, the naive bayes model performs well on larger dataset like Adult Dataset and Car Evaluation Dataset while the logistic regression model performs well on smaller dataset, including Iris Dataset and Ionosphere. we further investigate the influence of different learning rates, different stopping criteria, hyper-parameter tuning, feature selection and dataset selection.
=======
In this project, we implement two basic machine learning models: logistic regression and Naive Bayes. These models are tested upon four classification datasets: Ionosphere Dataset, Iris Dataset, Car Evaluation Dataset, and Adult Dataset. In the pre-processing phase, we first discretize all the continuous features, handling the missing values and values out of range. In the experiment phase, the Naive Bayes model performs well on larger datasets like Adult Dataset and Car Evaluation Dataset while the logistic regression model performs on smaller datasets, including Iris Dataset and Ionosphere. we further investigate the influence of different learning rates, different stopping criteria, hyper-parameter tuning, feature selection, and dataset selection.
>>>>>>> 73694987924d66bb667e95294c7048c2e0654013

\end{abstract}

\section{Introduction}
Classification tasks are one of the most common and important tasks in the machine learning community. Given certain features, the classification task is to categorize which class is most likely to be. In this project, we implement two basic machine learning models: logistic regression and Naive Bayes, and test their performance upon four datasets: Ionosphere Dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/ionosphere}}, Adult Dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Adult}}, Iris Dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Iris}} and Car Evaluation Dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Car+Evaluation}}. The logistic regression model performs better on Iris Dataset and Ionosphere Dataset, while the Naive Bayes model performs better on Adult Dataset and Car Evaluation Dataset. All the features are discretized during the pre-processing phase. We further investigate the influence of different training techniques, such as hyper-parameter tuning and feature selections, upon the performance of the models.


\section{Datasets}
The targets of all these four datasets are categorical classification (including binary classification). We exam four datasets to find that no missing values exist. We use \textit{replace} function in \textit{pandas} module to process all the target values into categorical count value. We will briefly describe the specific features and then introduce how to extract/process the features.

\subsubsection*{Ionosphere Dataset}
Ionosphere dataset contains 34 radar data (real values between -1 and 1) with 351 rows\cite{sigillito1989classification}. We find their distribution with respect to the "good" or "bad" target, which is reflected in Figure \ref{iono_feat}. We then remove radar 1 feature which is always 0. We can see that good class and bad class have quite distinct patterns in the antennas power value, which is essential in the following classification. We also fill between the intervals of good-class feature distribution to find that such surface is quite similar to the auto-correlated signals. Therefore, we use Principal Component Analysis (PCA) to reduce the dimensions from 34 into 10 to learn the low-rank representation for more efficient and powerful training.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\linewidth]{fig/iono_feat_dist.pdf}
	\caption{Distribution of average numerical features given "good" or "bad" class in ionosphere dataset}
	\label{iono_feat}
\end{figure}

\subsubsection*{Adult Dataset}
Adult dataset aims to predict whether income exceeds \$50K/yr based on census data\cite{kohavi1996scaling}. There are 32561 records and 14 features included with datatypes of continuous count values, continuous real values and categorical/binary values. We min-max normalize continuous values (both count and real),  discretize normalized real values into 10 categories and leave everything else untouched.


\subsubsection*{Iris Dataset}
There are 4 continuous real value attributes, whose basic statistic information are listed in Figure \ref{iris_des} and Table \ref{iris_table}, where \textit{sl,sw,pw,pw} stand for \textit{sepal length, sepal width, petal width, petal length} \cite{fisher1936use}. Linear correlation between features can be found, e.g. petal length \& petal width and petal length \& sepal length. We also min-max, normalize and then discretize the normed real values.

\begin{figure}[H]
	\begin{minipage}{0.9\linewidth}
		\begin{minipage}[b]{0.5\linewidth}
		  \centering
		  \includegraphics[width= \linewidth]{fig/iris_pairplot.pdf}
		  \captionof{figure}{Pairplot of iris data features}
		  \label{iris_des}
		\end{minipage}
		\hfill
		\begin{minipage}[b]{0.4\linewidth}
		  \centering
			\begin{tabular}{c|cccc}
				\hline
				 & sl & sw & pl & pw \\
				\hline
				count & 150 & 150 & 150 & 150\\
				mean & 5.84 & 3.05 & 3.75 & 1.19\\
				std & 0.82 & 0.43 & 1.76 & 0.76\\
				min & 4.3 & 2.0 & 1.0 & 0.1 \\
				25\% & 5.1 & 2.8 & 1.6 & 0.3\\
				50\% & 5.8 & 3.0 & 4.3 & 1.3\\
				75\% & 6.4 & 3.3 & 5.1 & 1.8 \\
				max & 7.9 & 4.4  & 6.9 & 2.5 \\
				\hline
			\end{tabular} 
			
		\captionof{table}{Iris data description}
		\label{iris_table}
	\end{minipage}
\end{minipage}

\end{figure}


\subsubsection*{Car Evaluation Dataset}
In Car Evaluation Dataset, we use 1728 records from 6 categorical features of cars, e.g. price, door number and capacity, to predict the safety level of the cars \cite{bohanec1988knowledge}. This dataset is straightforward, we only transform the raw data into categorical features to predict the categorical safety level.


\section{Results}
\subsection*{\underline{Accuracy comparison on four datasets}}

From Table \ref{accuracy_fourdata} , for accuracy of each model, we find that our implemented logistic regression model can work well on small datasets (iris and ionosphere datasets) but worse in larger dataset (car and adult datasets) compared with \textit{sklearn} model. In the contrary, naive bayes work well on large dataset but worse in small dataset.

\begin{table}[H]
	\centering
	\begin{tabular}{c|cccc}
		\hline
		 & iris	 & car & adult & ionosphere \\
		\hline
		logistic regression(ours) &  \textbf{0.75} & 0.65 & 0.66 & \textbf{0.80}\\
		logistic regression(sklearn) & 0.93 & 0.87 & 0.82 & 0.82\\
		naive bayes(ours) & 0.34 &  \textbf{0.69} & \textbf{0.76} & 0.67 \\
		naive bayes(sklearn) & 0.74 & 0.69 & 0.75 & 0.81 \\
		\hline
	\end{tabular} 
	\caption{Accuracy of our implemented model in four datasets comparing to sklearn module}
	\label{accuracy_fourdata}
\end{table}

Furthermore, we compare the different evaluation metrics for binary classification, where ionosphere dataset and adult dataset are selected. Details can be found in Table \ref{binary_comparison}, where \textbf{o} stands for our model and \textbf{s} for results after running \textit{sklearn} module.We found that our model have lower precision and recall on class 1 for large dataset (adult data), which can explain why the model fail to show the performance like \textit{sklearn} model. We plan to investigate more in the characteristic of the features in large dataset.

\begin{table}[H]

	\begin{tabular}{c|ccccccc}
		\hline
		& &  & ionosphere/adult & dataset & & & \\
		\hline
		& precision(o) & precision(s) & recall(o) & recall(s) & f1(o) & f1(s) & support \\
		\hline
		class 0 &1.00/0.93 &0.90/0.83 &0.48/0.49 &0.48/0.95 &0.65/0.64 &0.86/0.89 &33/4937\\
		class 1 &0.69/0.35 &0.85/0.71 &1.00/0.88 &1.00/0.39 &0.81/0.51&0.88/0.51&37/1575\\
		macro avg &0.84/0.64 &0.88/0.77 &0.74/0.68 &0.74/0.67 &0.73/0.57&0.87/0.70&70/6512\\
		weighted avg &0.83/0.79 &0.87/0.80 &0.76/0.58 &0.76/0.81  &0.74/0.61 &0.87/0.79&70/6512\\

		\hline
	\end{tabular} 
	\caption{Different evaluation metrics of our implemented model in binary-target datasets comparing to sklearn module}
	\label{binary_comparison}
\end{table}


\begin{figure}[H]
	\centering
	\begin{minipage}{0.8\linewidth}
		\begin{minipage}[b]{0.48\linewidth}
		  \centering
		  \includegraphics[width= \linewidth]{fig/iris-lr0001.png}
		\end{minipage}
		\hfill
		\begin{minipage}[b]{0.48\linewidth}
		  \centering			
		  \includegraphics[width= \linewidth]{fig/iris-lr-001.png}
		\end{minipage}
		\begin{minipage}[b]{0.48\linewidth}
			\centering
			\includegraphics[width= \linewidth]{fig/iris-lr-01.png}
		  \end{minipage}
		  \hfill
		  \begin{minipage}[b]{0.48\linewidth}
			\centering			
			\includegraphics[width= \linewidth]{fig/iris-lr-1.png}
		  \end{minipage}
	\end{minipage}
	\caption{Learning rate test, from left top to right top, from left bottom to right bottom, the learning rates are: 0.001, 0.01, 0.1 and 1}
	\label{lr_test}
\end{figure}

\subsection*{\underline{Learning rate test}}
We mainly test learning rates with stopping threshold 0.05 in four datasets, a specific case on iris dataset is shown in Figure \ref{lr_test}. For small learning rate, like 0.0001, the accuracy makes no change during the gradient decent, which means that it converges very slow. When the learning rate is very big such as 1 , the accuracy will fluctuate sharply, which means that it will become unstable during gradient descent. 


\begin{figure}[H]
	\centering
	\begin{minipage}{0.7\linewidth}
		\begin{minipage}[b]{0.48\linewidth}
		  \centering
		  \includegraphics[width=\linewidth]{fig/ionosphere_lr.pdf}
		\end{minipage}
		\hfill
		\begin{minipage}[b]{0.48\linewidth}
		  \centering			
		  \includegraphics[width= \linewidth]{fig/adult_lr.pdf}
		\end{minipage}
		\begin{minipage}[b]{0.48\linewidth}
			\centering
			\includegraphics[width= \linewidth]{fig/iris_lr.pdf}
		  \end{minipage}
		  \hfill
		  \begin{minipage}[b]{0.48\linewidth}
			\centering			
			\includegraphics[width= \linewidth]{fig/car_lr.pdf}
		  \end{minipage}
	\end{minipage}
	\caption{Hyperparameter search for learning rates in different datasets, from left bottom to right bottom, the learning rates are: ionosphere, adult, iris and car datasets}
	\label{lr_test}
\end{figure}


We also find similar effect on four data set, with different learning rate ranges. For example, in the iris dataset, the best learning rate in our experiment should be around 0.01. But in adult data set it should be 0.1. Then, we perform hyper-parameter search in a smaller range of learning rates, which is shown in Figure \ref{stopping_cri_lr}. According to our experiment, we find the best learning rate for the iris dataset should be 0.025, the ionosphere dataset should be 0.0025, the car evaluation dataset should be 0.015 and the adult dataset should be 0.24.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{fig/stopping-threshold.png}
	\caption{Best stopping criterion for Ionosphere Dataset and Iris Dataset given optimal learning rate}
	\label{stopping_cri_lr}
\end{figure}

\subsection*{\underline{Stopping criteria for logistic regression}}

We also investigate the stopping criteria on four datasets. The stopping criteria are considered as a threshold for change in the value of cost function. From Figure \ref{stopping_cri_lr}, we find there is not much influence on accuracy for car and adult datasets because these two datasets are very big, our model may need to run max iteration to meet the optimal solution. We find that for iris and ionosphere datasets, there will be optimal criteria, which are 0.15 and 0.14 respectively.



\subsection*{\underline{Accuracy as a function of the size of dataset}}


From Figure \ref{size_accuracy} we can find that accuracy increases when sample size is roughly less than 200. However, the accuracy becomes stable even the sample size continues growing. 

\begin{figure}[H]
	\centering
	\begin{minipage}{\linewidth}
		\begin{minipage}[b]{0.5\linewidth}
		  \centering
		  \includegraphics[width= \linewidth]{fig/dataset_size.png}
		  \captionof{figure}{Learning rate test, from left top to right top, from left bottom to right bottom, the learning rates are: 0.001, 0.01, 0.1 and 1}
		  \label{size_accuracy}
		\end{minipage}
		\hfill
		\begin{minipage}[b]{0.48\linewidth}
		  \centering			
		  \includegraphics[width= \linewidth]{fig/PCA.png}
		  \captionof{figure}{Training and validation accuracy comparison before and after using PCA}
		  \label{pca}
		\end{minipage}
	\end{minipage}
\end{figure}







\subsection*{\underline{Dimension reduction comparison}}
Because the ionosphere dataset has strong auto-correlation, we use PCA to reduce the dimensions for better training results, which can be reflected in Figure \ref{pca}. It can be found that accuracy on both training and validation sets perform better if PCA is applied. By using PCA for dimension reduction, we can learn a better representation of features.


\section{Discussion and Conclusion}

In this report, we implement logistic regression and Naive Bayes models. They are tested on four categorical classification datasets. In the data cleaning part, we check missing values, discretize the real values and reduce dimensions for auto-correlated features. We test the accuracy and impact of learning rate, stopping criteria and dimension reduction, where we all choose an improved strategy. As future work, we still need to look into why our model would perform less accuracy and efficient than \textit{sklearn} module.

\section{Statement of Contributions}

\begin{itemize}
	\item Dingyi Zhunag: Data preprocessing, data analysis and report formating.
	\item Fuyuan Lyu: Model implementation and report writing.
	\item Tianyu Shi: Running experiment, data visualization and data analysis.
\end{itemize}

\bibliographystyle{unsrt}
\bibliography{a1}

\end{document}
