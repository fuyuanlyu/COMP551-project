{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T18:11:58.636363Z",
     "start_time": "2020-02-29T18:11:56.521268Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import  AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from dataset.dataset import get_twenty_dataset, get_IMDB_dataset\n",
    "from main_dataset import main\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T18:12:10.901074Z",
     "start_time": "2020-02-29T18:11:58.638297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# We compare the condition where datasets are\n",
    "# 1. tf-idf\n",
    "# 2. tf-idf + remove_stopping_word\n",
    "# 3. tf-idf + remove_stopping_word + PCA (k=2,3,4,5,6)\n",
    "twenty_dataset_dict = {}\n",
    "IMDB_dataset_dict = {}\n",
    "\n",
    "# 1.\n",
    "twenty_dataset_dict[0]= list(get_twenty_dataset())\n",
    "IMDB_dataset_dict [0] = list(get_IMDB_dataset())\n",
    "\n",
    "# 2.\n",
    "twenty_dataset_dict[1]= list(get_twenty_dataset(remove_stop_word=True))\n",
    "IMDB_dataset_dict [1] = list(get_IMDB_dataset(remove_stop_word=True))\n",
    "\n",
    "# 3. \n",
    "for i in range(2,7):\n",
    "    print(i)\n",
    "    twenty_dataset_dict[i]= list(get_twenty_dataset(remove_stop_word=True,preprocessing_trick='PCA',n_components=i))\n",
    "    IMDB_dataset_dict [i] = list(get_IMDB_dataset(remove_stop_word=True,preprocessing_trick='PCA',n_components=i)) \n",
    "# x_train_20, y_train_20, x_test_20, y_test_20 = get_twenty_dataset()\n",
    "# x_train_imdb, y_train_imdb, x_test_imdb, y_test_imdb = get_IMDB_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = ['tf-idf','tf-idf, remove stopping word','PCA-2','PCA-3','PCA-4','PCA-5','PCA-6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 news results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T18:13:05.749245Z",
     "start_time": "2020-02-29T18:12:10.903032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomialNB model\n",
      "0.7320764737121614\n",
      "logistic regression model\n",
      "Decision Tree model\n",
      "0.46999468932554433\n",
      "SVM model\n",
      "0.8036378120021243\n",
      "AdaBoost model\n",
      "0.5\n",
      "Random forest model\n",
      "0.688396176314392\n",
      "MLPClassifier model\n",
      "0.7705788635156665\n"
     ]
    }
   ],
   "source": [
    "model_dict_20=main(x_train_20, y_train_20, x_test_20, y_test_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T18:14:50.605241Z",
     "start_time": "2020-02-29T18:13:05.751203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomialNB model\n",
      "0.82956\n",
      "logistic regression model\n",
      "Decision Tree model\n",
      "0.70416\n",
      "SVM model\n",
      "0.8772\n",
      "AdaBoost model\n",
      "0.83\n",
      "Random forest model\n",
      "0.8366\n",
      "MLPClassifier model\n",
      "0.87668\n"
     ]
    }
   ],
   "source": [
    "model_dict_imdb=main(x_train_imdb, y_train_imdb, x_test_imdb, y_test_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "clf_NB = MultinomialNB()\n",
    "clf_LR = LogisticRegression()\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_SVC = LinearSVC()\n",
    "clf_ADB = AdaBoostClassifier()\n",
    "clf_RDF = RandomForestClassifier()\n",
    "clf_NN = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(30,1024),max_iter=1) # Computationally heavy to tune NN\n",
    "clf_XG = XGBClassifier()\n",
    "\n",
    "#models_dict = {'NB':clf_NB,'LR': clf_LR,'DT':clf_DT,'SVC':clf_SVC,'ADB':clf_ADB,'RDF':clf_RDF,'NN':clf_NN,'XG':clf_XG}\n",
    "models_dict = {'NN':clf_NN}\n",
    "models_list = list(models_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tunable parameters ranges for different classifiers\n",
    "para_NB = {'NB__alpha':np.arange(0,2,0.2),'NB__fit_prior':[True,False]}\n",
    "para_LR = {'LR__penalty':['l1','l2','elasticnet'],\n",
    "           'LR__warm_start':[True,False]}\n",
    "para_DT = {'DT__criterion':['gini','entropy'],\n",
    "           'DT__max_features': ['sqrt','log2',None]}\n",
    "para_SVC = {'SVC__penalty': ['l1','l2'],\n",
    "            'SVC__loss': ['hinge','squared_hinge']}\n",
    "para_ADB = {'ADB__n_estimators': [30,50,80,100,120],\n",
    "            'ADB__learning_rate':np.arange(0,1.2,0.2)}\n",
    "para_RDF = {'RDF__criterion':['gini','entropy'],\n",
    "            'RDF__max_features': ['sqrt','log2',None]}\n",
    "para_NN = {'NN__solver':['lbfgs'], 'NN__hidden_layer_sizes' :[(30,1024)],'NN__max_iter':[1] }\n",
    "para_XG = {'XG__learning_rate':[0.1]}\n",
    "#para_NN = {'NN__hidden_layer_sizes':[(5,),(12,),(25,),(50,),(100,)],\n",
    "#           'NN__activation':['identity','logistic','tanh','relu'],\n",
    "#           'NN__solver':['lbfgs','sgd','adam'],\n",
    "#           'NN__learning_rate':['constant','invscaling','adaptive']}\n",
    "#params_list = [para_NB,para_LR,para_DT,para_SVC,para_ADB,para_RDF,para_NN,para_XG]\n",
    "params_list = [para_NN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print( len(models_list) == len(params_list) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model specifics which are the same for both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b86a2ff4014917adf8e1f6e6d41613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Model  NN  for 20 news  tf-idf score = 0.0539\n",
      "Model  NN  for 20 news   tf-idf best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "----------\n",
      "Model  NN  for IMDB  tf-idf  score = 0.5044\n",
      "Model  NN  for IMDB  tf-idf  best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "Model  NN  for 20 news  tf-idf, remove stopping word score = 0.0524\n",
      "Model  NN  for 20 news   tf-idf, remove stopping word best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "----------\n",
      "Model  NN  for IMDB  tf-idf, remove stopping word  score = 0.5000\n",
      "Model  NN  for IMDB  tf-idf, remove stopping word  best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "Model  NN  for 20 news  PCA-2 score = 0.0526\n",
      "Model  NN  for 20 news   PCA-2 best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "----------\n",
      "Model  NN  for IMDB  PCA-2  score = 0.5000\n",
      "Model  NN  for IMDB  PCA-2  best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "Model  NN  for 20 news  PCA-3 score = 0.0421\n",
      "Model  NN  for 20 news   PCA-3 best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "----------\n",
      "Model  NN  for IMDB  PCA-3  score = 0.5012\n",
      "Model  NN  for IMDB  PCA-3  best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "Model  NN  for 20 news  PCA-4 score = 0.0920\n",
      "Model  NN  for 20 news   PCA-4 best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "----------\n",
      "Model  NN  for IMDB  PCA-4  score = 0.5027\n",
      "Model  NN  for IMDB  PCA-4  best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "Model  NN  for 20 news  PCA-5 score = 0.0705\n",
      "Model  NN  for 20 news   PCA-5 best params {'NN__hidden_layer_sizes': (30, 1024), 'NN__max_iter': 1, 'NN__solver': 'lbfgs'}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# No preprocessing scalar added\n",
    "grid_dict_20 = {}\n",
    "grid_dict_imdb = {}\n",
    "scores = np.zeros((2,len(models_list),len(dataset_list)))\n",
    "\n",
    "k = 5 # Magic number\n",
    "n_jobs =5\n",
    "\n",
    "for idx,model_item in tqdm_notebook(enumerate(models_list),total=len(models_list)):\n",
    "    pipline = Pipeline([model_item])\n",
    "    print('#'*20)\n",
    "    # For 20 news\n",
    "    grid_20 = GridSearchCV(pipline,param_grid=params_list[idx],cv=k,\n",
    "                        error_score=0.0,n_jobs=n_jobs)\n",
    "    # For imdb\n",
    "    grid_imdb = GridSearchCV(pipline,param_grid=params_list[idx],cv=k,\n",
    "                        error_score=0.0,n_jobs=n_jobs)\n",
    "    \n",
    "    for dataset_idx in range(len(twenty_dataset_dict)):\n",
    "        grid_20.fit(twenty_dataset_dict[dataset_idx][0],twenty_dataset_dict[dataset_idx][1])\n",
    "        scores[0,idx,dataset_idx] = grid_20.score(twenty_dataset_dict[dataset_idx][2], twenty_dataset_dict[dataset_idx][3])\n",
    "        print('Model ',model_item[0],' for 20 news ' , dataset_list[dataset_idx] ,'score = %3.4f'%(scores[0,idx,dataset_idx]))\n",
    "        print('Model ',model_item[0],' for 20 news  ' , dataset_list[dataset_idx] ,'best params', grid_20.best_params_)\n",
    "        grid_dict_20[model_item[0]] = grid_20\n",
    "         \n",
    "        \n",
    "    \n",
    "        grid_imdb.fit(IMDB_dataset_dict[dataset_idx][0],IMDB_dataset_dict[dataset_idx][1])\n",
    "        scores[1,idx,dataset_idx] = grid_imdb.score(IMDB_dataset_dict[dataset_idx][2], IMDB_dataset_dict[dataset_idx][3])\n",
    "        print('Model ',model_item[0],' for IMDB ' , dataset_list[dataset_idx] ,' score = %3.4f'%(scores[1,idx,dataset_idx]))\n",
    "        print('Model ',model_item[0],' for IMDB ' , dataset_list[dataset_idx] ,' best params', grid_imdb.best_params_)\n",
    "        grid_dict_imdb[model_item[0]] = grid_imdb\n",
    "        print('-'*10)\n",
    "    \n",
    "joblib.dump(grid_dict_20,'grid_dict_20.asv')\n",
    "joblib.dump(grid_dict_imdb,'grid_dict_imdb.asv')\n",
    "joblib.dump(scores,'scores.asv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
