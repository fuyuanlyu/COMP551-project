{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T18:11:58.636363Z",
     "start_time": "2020-02-29T18:11:56.521268Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import  AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from dataset.dataset import get_twenty_dataset, get_IMDB_dataset\n",
    "from main_dataset import main\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T18:12:10.901074Z",
     "start_time": "2020-02-29T18:11:58.638297Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_20, y_train_20, x_test_20, y_test_20 = get_twenty_dataset()\n",
    "x_train_imdb, y_train_imdb, x_test_imdb, y_test_imdb = get_IMDB_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 news results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T18:13:05.749245Z",
     "start_time": "2020-02-29T18:12:10.903032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomialNB model\n",
      "0.7320764737121614\n",
      "logistic regression model\n",
      "Decision Tree model\n",
      "0.46999468932554433\n",
      "SVM model\n",
      "0.8036378120021243\n",
      "AdaBoost model\n",
      "0.5\n",
      "Random forest model\n",
      "0.688396176314392\n",
      "MLPClassifier model\n",
      "0.7705788635156665\n"
     ]
    }
   ],
   "source": [
    "model_dict_20=main(x_train_20, y_train_20, x_test_20, y_test_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T18:14:50.605241Z",
     "start_time": "2020-02-29T18:13:05.751203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomialNB model\n",
      "0.82956\n",
      "logistic regression model\n",
      "Decision Tree model\n",
      "0.70416\n",
      "SVM model\n",
      "0.8772\n",
      "AdaBoost model\n",
      "0.83\n",
      "Random forest model\n",
      "0.8366\n",
      "MLPClassifier model\n",
      "0.87668\n"
     ]
    }
   ],
   "source": [
    "model_dict_imdb=main(x_train_imdb, y_train_imdb, x_test_imdb, y_test_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "clf_NB = MultinomialNB()\n",
    "# clf_LR = LogisticRegression()\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_SVC = LinearSVC()\n",
    "clf_ADB = AdaBoostClassifier()\n",
    "clf_RDF = RandomForestClassifier()\n",
    "clf_NN = MLPClassifier()\n",
    "\n",
    "models_dict = {'NB':clf_NB,'DT':clf_DT,'SVC':clf_SVC,'ADB':clf_ADB,'RDF':clf_RDF,'NN':clf_NN}\n",
    "#models_dict = {'NB':clf_NB,'LR':clf_LR}\n",
    "models_list = list(models_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tunable parameters ranges for different classifiers\n",
    "para_NB = {'NB__alpha':np.arange(0,2,0.2),'NB__fit_prior':[True,False]}\n",
    "# para_LR = {'LR__penalty':['l1','l2','elasticnet'],\n",
    "#           'LR__solver':['newton-cg','lbfgs','liblinear','sag','saga'], \n",
    "#           'LR__warm_start':[True,False]}\n",
    "para_DT = {'DT__criterion':['gini','entropy'],\n",
    "           'DT__max_features': ['sqrt','log2',None]}\n",
    "para_SVC = {'SVC__penalty': ['l1','l2'],\n",
    "            'SVC__loss': ['hinge','squared_hinge']}\n",
    "para_ADB = {'ADB__n_estimators': [30,50,80,100,120],\n",
    "            'ADB__learning_rate':np.arange(0,1.2,0.2)}\n",
    "para_RDF = {'RDF__criterion':['gini','entropy'],\n",
    "            'RDF__max_features': ['sqrt','log2',None]}\n",
    "para_NN = {'NN__hidden_layer_sizes':[(5,),(12,),(25,),(50,),(100,)],\n",
    "           'NN__activation':['identity','logistic','tanh','relu'],\n",
    "           'NN__solver':['lbfgs','sgd','adam'],\n",
    "           'NN__learning_rate':['constant','invscaling','adaptive']}\n",
    "params_list = [para_NB,para_DT,para_SVC,para_ADB,para_RDF,para_NN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print( len(models_list) == len(params_list) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model specifics which are the same for both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6a7622e0af49858974556a2deae291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=10)]: Done 200 out of 200 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  NB  for 20 news score = 0.7463\n",
      "Model  NB  for 20 news best params {'NB__alpha': 0.0, 'NB__fit_prior': True}\n",
      "----------\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  60 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=10)]: Done 200 out of 200 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  NB  for IMDB score = 0.8317\n",
      "Model  NB  for IMDB best params {'NB__alpha': 1.8, 'NB__fit_prior': True}\n",
      "####################\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=10)]: Done  60 out of  60 | elapsed:   57.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  DT  for 20 news score = 0.4810\n",
      "Model  DT  for 20 news best params {'DT__criterion': 'gini', 'DT__max_features': None}\n",
      "----------\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=10)]: Done  60 out of  60 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  DT  for IMDB score = 0.7037\n",
      "Model  DT  for IMDB best params {'DT__criterion': 'gini', 'DT__max_features': None}\n",
      "####################\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  40 out of  40 | elapsed:   24.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  SVC  for 20 news score = 0.8036\n",
      "Model  SVC  for 20 news best params {'SVC__loss': 'squared_hinge', 'SVC__penalty': 'l2'}\n",
      "----------\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 out of  40 | elapsed:    1.5s remaining:    1.4s\n",
      "[Parallel(n_jobs=10)]: Done  40 out of  40 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  SVC  for IMDB score = 0.8826\n",
      "Model  SVC  for IMDB best params {'SVC__loss': 'hinge', 'SVC__penalty': 'l2'}\n",
      "####################\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done 300 out of 300 | elapsed: 49.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  ADB  for 20 news score = 0.5046\n",
      "Model  ADB  for 20 news best params {'ADB__learning_rate': 0.6000000000000001, 'ADB__n_estimators': 100}\n",
      "----------\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done 155 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=10)]: Done 300 out of 300 | elapsed: 41.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  ADB  for IMDB score = 0.8382\n",
      "Model  ADB  for IMDB best params {'ADB__learning_rate': 0.8, 'ADB__n_estimators': 120}\n",
      "####################\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=10)]: Done  60 out of  60 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  RDF  for 20 news score = 0.5586\n",
      "Model  RDF  for 20 news best params {'RDF__criterion': 'gini', 'RDF__max_features': None}\n",
      "----------\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=10)]: Done  60 out of  60 | elapsed:  9.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  RDF  for IMDB score = 0.7762\n",
      "Model  RDF  for IMDB best params {'RDF__criterion': 'gini', 'RDF__max_features': None}\n",
      "####################\n",
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed: 158.0min\n"
     ]
    }
   ],
   "source": [
    "# No preprocessing scalar added\n",
    "grid_dict_20 = {}\n",
    "grid_dict_imdb = {}\n",
    "\n",
    "k = 10 # Magic number\n",
    "n_jobs =10\n",
    "\n",
    "for idx,model_item in tqdm_notebook(enumerate(models_list),total=len(models_list)):\n",
    "    pipline = Pipeline([model_item])\n",
    "    print('#'*20)\n",
    "    # For 20 news\n",
    "    grid = GridSearchCV(pipline,param_grid=params_list[idx],cv=k,\n",
    "                        error_score=0.0,n_jobs=n_jobs,verbose=1)\n",
    "    grid.fit(x_train_20, y_train_20)\n",
    "    print('Model ',model_item[0],' for 20 news score = %3.4f'%(grid.score(x_test_20, y_test_20)))\n",
    "    print('Model ',model_item[0],' for 20 news best params', grid.best_params_)\n",
    "    grid_dict_20[model_item[0]] = grid\n",
    "    print('-'*10)\n",
    "    # For imdb\n",
    "    grid = GridSearchCV(pipline,param_grid=params_list[idx],cv=k,\n",
    "                        error_score=0.0,n_jobs=n_jobs,verbose=1)\n",
    "    grid.fit(x_train_imdb, y_train_imdb)\n",
    "    print('Model ',model_item[0],' for IMDB score = %3.4f'%(grid.score(x_test_imdb, y_test_imdb)))\n",
    "    print('Model ',model_item[0],' for IMDB best params', grid.best_params_)\n",
    "    grid_dict_imdb[model_item[0]] = grid\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('scaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('NB',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'NB__alpha': array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8]),\n",
       "                         'NB__fit_prior': [True, False]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
